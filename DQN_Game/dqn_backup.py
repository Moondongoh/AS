import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import random
import matplotlib.pyplot as plt

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")


### í™˜ê²½ ì„¤ì • class
class GridEnvironment:
    ### í™˜ê²½ì„¤ì •
    def __init__(self):
        self.grid_size = 9                                      # ê²©ì í¬ê¸°
        self.grid = np.zeros((self.grid_size, self.grid_size))  # 9x9 í¬ê¸°ì˜ 0ìœ¼ë¡œ ì±„ì›Œì§„ ë°°ì—´ ìƒì„±
        self.start = (1, 1)                                     # ì‹œì‘ ì§€ì 
        #self.danger_zone = {'top_left': (0, 4), 'size': 5}      # 3,3ì—ì„œ ì‹œì‘í•˜ëŠ” 3x3 ì •ì‚¬ê°í˜•
        self.danger_zone = {'top_left': (3, 3), 'size': 3}      # 3,3ì—ì„œ ì‹œì‘í•˜ëŠ” 3x3 ì •ì‚¬ê°í˜•
        self.max_steps = 200                                    # ìµœëŒ€ ì´ë™ íšŸìˆ˜(ê°™ì€ ë°©í–¥ìœ¼ë¡œ ë„ˆë¬´ ë§ì´ ì§„í–‰í•  ê²½ìš° ë°©ì§€ì§€)
        self.current_steps = 0                                  # í˜„ì¬ ì´ë™ íšŸìˆ˜ 
        self.direction = 0                                      # ì´ˆê¸° ë°©í–¥ (0ë„ëŠ” ì˜¤ë¥¸ìª½, 90ë„ëŠ” ìœ„ìª½)    
        self.position = self.start                              # í˜„ì¬ ìœ„ì¹˜   
        self.last_angle = 0                                     # ë§ˆì§€ë§‰ ê°ë„ (ì´ì „ ì´ë™ ë°©í–¥)
        self.total_reward = 0                                   # ëˆ„ì  ë³´ìƒ ì¶”ì ì„ ìœ„í•´ ì¶”ê°€
        self.flag = False                                       # ìœ„í—˜ ì§€ì—­ ì§„ì… ì—¬ë¶€ í”Œë˜ê·¸    
        self.prev_dist = 0                                      # ì´ì „ ìœ„ì¹˜ì™€ì˜ ê±°ë¦¬
        self.visited_goals = []                                 # ë°©ë¬¸í•œ ëª©í‘œ ì§€ì  ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
        self.visited_positions = {}                             # ì¤‘ë³µ ë°©ë¬¸ ì²´í¬ìš© ë”•ì…”ë„ˆë¦¬
        self.direction_score = 0  # ë°©í–¥ ì ìˆ˜ ì´ˆê¸°í™”
        self.goal = (7, 7)  # ëª©í‘œ ì§€ì 
        
        self.recent_moves = []                                  # ìµœê·¼ 3ë²ˆì˜ íšŒì „ ê¸°ë¡



        self.reset()
        
    def _get_wall_distances(self):
        x, y = self.position
        top_dist = x
        bottom_dist = self.grid_size - 1 - x
        left_dist = y
        right_dist = self.grid_size - 1 - y
        return top_dist, bottom_dist, left_dist, right_dist

    
    def reset(self):
        self.direction_score = 0  # ë°©í–¥ ì ìˆ˜ ì´ˆê¸°í™”
        self.position = self.start              # ì‹œì‘ ìœ„ì¹˜ë¡œ ì´ˆê¸°í™”
        self.direction = 0                      # ë°©í–¥ ì´ˆê¸°í™”
        self.current_steps = 0                  # ì´ë™ íšŸìˆ˜ ì´ˆê¸°í™”
        self.last_angle = np.arctan2(0, 0)      # ë§ˆì§€ë§‰ ê°ë„ ì´ˆê¸°í™”
        self.total_reward = 0                   # ë¦¬ì…‹ ì‹œ ëˆ„ì  ë³´ìƒë„ ì´ˆê¸°í™”
        self.prev_dist = 0                      # ì´ì „ ìœ„ì¹˜ì™€ì˜ ê±°ë¦¬ ì´ˆê¸°í™”                                                    
        self.flag = False                       # ìœ„í—˜ ì§€ì—­ ì§„ì… ì—¬ë¶€ í”Œë˜ê·¸ ì´ˆê¸°í™”
        self.visited_goals = []                 # ë°©ë¬¸í•œ ëª©í‘œ ì§€ì  ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
        
        self.recent_moves = []                  # ìµœê·¼ 3ë²ˆì˜ íšŒì „ ê¸°ë¡ ì´ˆê¸°í™”
        
        return self._get_state()                # ì´ˆê¸° ìƒíƒœ ë°˜í™˜

    def _is_in_square(self, position):
        x, y = position
        x0, y0 = self.danger_zone['top_left']
        size = self.danger_zone['size']
        return x0 <= x < x0 + size and y0 <= y < y0 + size
    
    def _distance_to_boundary(self, position, angle):
        x, y = position
        dx, dy = np.cos(angle), np.sin(angle)

        t_x = (0 - x) / dx if dx < 0 else (self.grid_size - 1 - x) / dx if dx > 0 else float('inf')
        t_y = (0 - y) / dy if dy < 0 else (self.grid_size - 1 - y) / dy if dy > 0 else float('inf')

        distance_to_boundary = min(max(t_x, 0), max(t_y, 0))
        return distance_to_boundary
    
    def _distance_to_square(self, position, angle):
        x, y = position
        x0, y0 = self.danger_zone['top_left']
        size = self.danger_zone['size']

        dx, dy = np.cos(angle), np.sin(angle)

        t_vals = []
        for edge_x in [x0, x0 + size]:
            if dx != 0:
                t_vals.append((edge_x - x) / dx)

        for edge_y in [y0, y0 + size]:
            if dy != 0:
                t_vals.append((edge_y - y) / dy)

        t_vals = [t for t in t_vals if t > 0]
        return min(t_vals) if t_vals else float('inf')

    def _distance_in_direction(self, angle):
        radians = np.radians(angle)
        dist_to_boundary = self._distance_to_boundary(self.position, radians)
        dist_to_danger = self._distance_to_square(self.position, radians)
        return min(dist_to_boundary, dist_to_danger)

    def _get_state(self):
        directions = [-90, -45, 0, 45, 90]
        state_info = [
            self._distance_in_direction(self.direction + angle) / self.grid_size
            for angle in directions
        ]

        dx = self.position[0] - self.start[0]
        dy = self.position[1] - self.start[1]
        dist_to_start = np.sqrt(dx**2 + dy**2) / self.grid_size
        current_angle = np.arctan2(dy, dx) / np.pi
        step_info = self.current_steps / self.max_steps
        
        # ë²½ê¹Œì§€ ê±°ë¦¬ ì •ë³´ ì¶”ê°€
        top_dist, bottom_dist, left_dist, right_dist = self._get_wall_distances()
        wall_info = [top_dist / self.grid_size, bottom_dist / self.grid_size,
                 left_dist / self.grid_size, right_dist / self.grid_size]

        return np.array(state_info + [dist_to_start, current_angle, step_info] + wall_info)

    def step(self, action):
        rotations = [-45, 0, 45]
        angle_change = rotations[action]

        # ë°©í–¥ ê°±ì‹ 
        self.direction = (self.direction + angle_change) % 360
        if self.direction == -45:
            self.direction = 315
            
        # ğŸ§  ë²½ ê·¼ì²˜ íšŒí”¼: ì´ë™ ì „ ë°©í–¥ ì¡°ì •
        wall_threshold = 1  # 1ì¹¸ ì´ë‚´ë©´ íšŒí”¼
        top_dist, bottom_dist, left_dist, right_dist = self._get_wall_distances()

        if self.direction == 0 and right_dist <= wall_threshold:
            self.direction = 315 if random.random() < 0.5 else 45
        elif self.direction == 45 and (top_dist <= wall_threshold or right_dist <= wall_threshold):
            self.direction = 0
        elif self.direction == 315 and (bottom_dist <= wall_threshold or right_dist <= wall_threshold):
            self.direction = 0

        # ì´ë™ ë°©í–¥ ë§µí•‘ (ì •ìˆ˜ ê²©ì ê¸°ë°˜ìœ¼ë¡œ)
        move_map = {
            0:   (0, 1),    # ì˜¤ë¥¸ìª½
            45:  (-1, 1),   # ì˜¤ë¥¸ìª½ ìœ„
            315: (1, 1),    # ì˜¤ë¥¸ìª½ ì•„ë˜
        }
        
        # ì´ë™ ì „ ìœ„ì¹˜ ì €ì¥
        prev_position = self.position

        dx, dy = move_map.get(self.direction, (0, 0))  # ì•ˆì „ ì²˜ë¦¬
        new_x = int(np.clip(self.position[0] + dx, 0, self.grid_size - 1))
        new_y = int(np.clip(self.position[1] + dy, 0, self.grid_size - 1))
        self.position = (new_x, new_y)
        self.current_steps += 1
        
        # âœ… ì œìë¦¬ë©´ penalty ë˜ëŠ” ì¢…ë£Œ
        if self.position == prev_position:
            reward = -10  # ê°•í•œ íŒ¨ë„í‹°
            self.total_reward += reward
            return self._get_state(), reward, True  # ì¦‰ì‹œ ì¢…ë£Œ (ì„ íƒ)

        # ê¸°ë³¸ ë³´ìƒ
        reward = 1

        # # ë°©í–¥ ì ìˆ˜ ì—…ë°ì´íŠ¸
        # if angle_change == 0:
        #     self.direction_score = 0  # â† â— ì§ì§„ë„ ì ìˆ˜ ëˆ„ì  ì¤‘
        # else:
        #     self.direction_score += int(np.sign(angle_change))
        #     reward += 1  # âœ… ë°©í–¥ ì „í™˜ ì‹œ ì†ŒëŸ‰ ë³´ìƒ
        
        # ë°©í–¥ ì ìˆ˜ ì—…ë°ì´íŠ¸
        if angle_change != 0:
            self.recent_moves.append(int(np.sign(angle_change)))  # -1 or +1
        else:
            self.recent_moves.append(0)  # ì§ì§„ë„ ê¸°ë¡
            
        if len(self.recent_moves) > 3:
            self.recent_moves.pop(0)
            
        if len(self.recent_moves) == 3 and sum(self.recent_moves) in [3, -3]:
            print(f"ğŸš« ìµœê·¼ íšŒì „ì´ í•œìª½ìœ¼ë¡œ 3ë²ˆ ëˆ„ì ë¨: {self.recent_moves} â†’ ì¢…ë£Œ")
            return self._get_state(), reward, True
        # ì œìë¦¬ë©´ ì¢…ë£Œ
        
        if self.position == prev_position:
            print(f"ğŸ›‘ ì¢…ë£Œ: ì œìë¦¬ ì´ë™ ê°ì§€ at {self.position}")
            reward = -10
            self.total_reward += reward
            return self._get_state(), reward, True


        # ëª©í‘œ ë„ë‹¬
        if np.linalg.norm(np.array(self.position) - np.array(self.goal)) < 0.5:
            print(f"ğŸ ì¢…ë£Œ: ëª©í‘œ ë„ë‹¬ at {self.position}")
            reward += 10
            self.total_reward += reward
            return self._get_state(), reward, True

        # ë°©í–¥ ì ìˆ˜ ëˆ„ì  (ì˜ˆì „ ë°©ì‹ ìœ ì§€ ì¤‘ì´ë©´)
        if abs(self.direction_score) >= 3:
            print(f"ğŸ›‘ ì¢…ë£Œ: ë°©í–¥ ëˆ„ì  ì ìˆ˜ ì´ˆê³¼: {self.direction_score}")
            self.total_reward += reward
            return self._get_state(), reward, True

        # ìœ„í—˜ ì§€ì—­ ë„ë‹¬
        if self._is_in_square(self.position):
            print(f"ğŸ›‘ ì¢…ë£Œ: ìœ„í—˜ ì§€ì—­ ì§„ì… at {self.position}")
            self.total_reward += reward
            return self._get_state(), reward, True

        # ë²½ ë„ë‹¬
        if self.position[0] in [0, self.grid_size - 1] or self.position[1] in [0, self.grid_size - 1]:
            print(f"ğŸ›‘ ì¢…ë£Œ: ë²½ ë„ë‹¬ at {self.position}")
            self.total_reward += reward
            return self._get_state(), reward, True

        # ëˆ„ì  ë¦¬ì›Œë“œ ë„ˆë¬´ ë‚®ìŒ
        if self.total_reward <= -100:
            print(f"ğŸ›‘ ì¢…ë£Œ: ëˆ„ì  ë¦¬ì›Œë“œ {self.total_reward} ì´í•˜")
            return self._get_state(), reward, True

        # ìµœëŒ€ ìŠ¤í… ì´ˆê³¼
        done = self.current_steps >= self.max_steps
        if done:
            print(f"ğŸ›‘ ì¢…ë£Œ: ìµœëŒ€ ìŠ¤í… ì´ˆê³¼ {self.current_steps}")
            return self._get_state(), reward, done

        

        # ëª©í‘œ ë„ë‹¬ í™•ì¸
        if np.linalg.norm(np.array(self.position) - np.array(self.goal)) < 0.5:
            reward += 10
            self.total_reward += reward
            return self._get_state(), reward, True

        # ë°©í–¥ ì œí•œ ì¡°ê±´
        if abs(self.direction_score) >= 3:
            self.total_reward += reward
            return self._get_state(), reward, True

        # ìœ„í—˜ì§€ì—­ ë„ë‹¬
        if self._is_in_square(self.position):
            self.total_reward += reward
            return self._get_state(), reward, True

        # ë²½(ê²½ê³„) ë„ë‹¬
        if self.position[0] in [0, self.grid_size - 1] or self.position[1] in [0, self.grid_size - 1]:
            self.total_reward += reward
            return self._get_state(), reward, True

        # ì‹œì‘ì ê³¼ì˜ ê±°ë¦¬ ê¸°ë°˜ ë³´ìƒ
        dx_start = self.position[0] - self.start[0]
        dy_start = self.position[1] - self.start[1]
        dist_to_start = np.sqrt(dx_start ** 2 + dy_start ** 2)

        if dist_to_start > self.prev_dist:
            reward += dist_to_start
        else:
            reward -= 5

        self.prev_dist = dist_to_start
        self.total_reward += reward

        # ëˆ„ì  ë³´ìƒì´ ë„ˆë¬´ ë‚®ìœ¼ë©´ ì¢…ë£Œ
        if self.total_reward <= -100:
            return self._get_state(), reward, True

        # ìµœëŒ€ ìŠ¤í… ì´ˆê³¼
        done = self.current_steps >= self.max_steps
        return self._get_state(), reward, done

class DQN(nn.Module):

    def __init__(self, state_dim, action_dim=3):
        super(DQN, self).__init__()
        self.fc = nn.Sequential( 
            nn.Linear(state_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )

    def forward(self, x):
        return self.fc(x)

def train_dqn(env):
    state_dim = 12
    action_dim = 3
    
    model = DQN(state_dim, action_dim).to(device)
    target_model = DQN(state_dim, action_dim).to(device)
    target_model.load_state_dict(model.state_dict())
    
    optimizer = optim.Adam(model.parameters(), lr=0.0003)
    criterion = nn.MSELoss()

    replay_buffer = []
    max_buffer_size = 20000
    batch_size = 256
    gamma = 0.99
    epsilon = 1.0
    epsilon_decay = 0.999
    epsilon_min = 0.01
    episodes = 8000
    target_update = 10

    rewards_history = []

    for episode in range(episodes):
        state = env.reset()
        done = False
        total_reward = 0
        
        while not done:
            if random.random() < epsilon:
                action = random.randint(0, action_dim - 1)
            else:
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
                with torch.no_grad():
                    q_values = model(state_tensor)
                action = q_values.argmax().item()

            next_state, reward, done = env.step(action)
            total_reward += reward

            replay_buffer.append((state, action, reward, next_state, done))
            if len(replay_buffer) > max_buffer_size:
                replay_buffer.pop(0)

            state = next_state

            if len(replay_buffer) >= batch_size:
                batch = random.sample(replay_buffer, batch_size)
                states, actions, rewards, next_states, dones = zip(*batch)

                states = torch.FloatTensor(np.array(states)).to(device)
                next_states = torch.FloatTensor(np.array(next_states)).to(device)
                actions = torch.LongTensor(actions).unsqueeze(1).to(device)
                rewards = torch.FloatTensor(rewards).to(device)
                dones = torch.FloatTensor(dones).to(device)

                current_q_values = model(states).gather(1, actions).squeeze()

                with torch.no_grad():
                    next_q_values = target_model(next_states).max(1)[0]
                    target_q_values = rewards + (1 - dones) * gamma * next_q_values

                loss = criterion(current_q_values, target_q_values)
                
                optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()

        epsilon = max(epsilon * epsilon_decay, epsilon_min)
        
        if episode % target_update == 0:
            target_model.load_state_dict(model.state_dict())

        rewards_history.append(total_reward)
        
        if (episode + 1) % 100 == 0:
            avg_reward = sum(rewards_history[-10:]) / 10
            print(f"Episode {episode + 1}: Average Reward: {avg_reward:.2f}, Epsilon: {epsilon:.2f}")

    return model, rewards_history

def visualize_episode_steps(env, model, episode_num, fig=None, ax=None):
    if fig is None or ax is None:
        fig, ax = plt.subplots(figsize=(8, 8))
    
    state = env.reset()
    positions = [env.position]
    rewards = []
    states = [state]
    done = False

    ax.set_xlim(0, env.grid_size)
    ax.set_ylim(0, env.grid_size)
    ax.invert_yaxis() 
    ax.set_xticks(range(env.grid_size))
    ax.set_yticks(range(env.grid_size))
    ax.grid(True)

    while not done:
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
        with torch.no_grad():
            q_values = model(state_tensor)
        action = q_values.argmax().item()
        
        next_state, reward, done = env.step(action)
        positions.append(env.position)
        rewards.append(reward)
        states.append(next_state)
        state = next_state

    for step in range(len(positions)):
        ax.clear()
        x0, y0 = env.danger_zone['top_left']
        size = env.danger_zone['size']
        square = plt.Rectangle((y0, x0), size, size, edgecolor='red', facecolor='red', alpha=0.9)
        ax.add_patch(square)
        
        ax.set_xlim(0, env.grid_size)
        ax.set_ylim(0, env.grid_size)
        ax.invert_yaxis()
        ax.set_xticks(range(env.grid_size))
        ax.set_yticks(range(env.grid_size))
        ax.grid(True)

        current_positions = positions[:step+1]
        if current_positions:
            path_coords = np.array(current_positions)
            ax.plot(path_coords[:, 1], path_coords[:, 0], 'b-', alpha=0.5)
            for idx, (pos_x, pos_y) in enumerate(current_positions[:-1]):
                ax.plot(pos_y, pos_x, 'bo', alpha=0.3, markersize=8)

        ax.plot(env.start[1], env.start[0], 'go', markersize=15, label='Start')
        ax.plot(env.goal[1], env.goal[0], 'yx', markersize=15, label='Goal')

        current_x, current_y = positions[step]
        ax.plot(current_y, current_x, 'r*', markersize=15, label='Current')
        
        # âœ… ëª©í‘œ ë„ë‹¬ ì‹œ ì¼ì‹œì •ì§€
        if np.linalg.norm(np.array(positions[step]) - np.array(env.goal)) < 0.5:
            input(f"âœ… Goal reached at step {step}! Press Enter to continue...")
    
        cumulative_reward = sum(rewards[:step]) if step > 0 else 0
        ax.set_title(f'Episode {episode_num+1}, Step {step}/{len(positions)-1}\n'
                     f'Position: {positions[step]}, Reward: {cumulative_reward:.1f}\n ' f'state: {states[step]}')
        ax.legend()
        
        plt.draw()
        plt.pause(0.2)

def train_dqn_with_visualization(env):                              
    state_dim = 12
    action_dim = 3  
    
    model = DQN(state_dim, action_dim).to(device)
    target_model = DQN(state_dim, action_dim).to(device)
    target_model.load_state_dict(model.state_dict())
    
    optimizer = optim.Adam(model.parameters(), lr=0.0003)
    criterion = nn.MSELoss()

    replay_buffer = []
    max_buffer_size = 20000
    batch_size = 256
    gamma = 0.99
    epsilon = 1.0
    epsilon_decay = 0.999
    epsilon_min = 0.01
    episodes = 8000
    target_update = 10

    fig, ax1 = plt.subplots(1, 1, figsize=(8, 8))
    rewards_history = []
    steps_history = []
    plt.ion()

    for episode in range(episodes):
        state = env.reset()
        done = False
        total_reward = 0
        step_count = 0
        episode_terminated = False
        
        while not done and not episode_terminated:
            if random.random() < epsilon:
                action = random.randint(0, action_dim - 1)
            else:
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
                with torch.no_grad():
                    q_values = model(state_tensor)
                action = q_values.argmax().item()
            
            next_state, reward, done = env.step(action)
            total_reward += reward
            step_count += 1
            
            # ëˆ„ì  ë¦¬ì›Œë“œê°€ -50 ì´í•˜ë©´ ì—í”¼ì†Œë“œ ì¢…ë£Œ
            if total_reward <= -100:
                episode_terminated = True
                done = True
            
            # Store in replay buffer
            replay_buffer.append((state, action, reward, next_state, done))
            if len(replay_buffer) > max_buffer_size:
                replay_buffer.pop(0)

            state = next_state

            # Train model if buffer has enough samples
            if len(replay_buffer) >= batch_size:
                batch = random.sample(replay_buffer, batch_size)
                states, actions, rewards, next_states, dones = zip(*batch)

                states = torch.FloatTensor(np.array(states)).to(device)
                next_states = torch.FloatTensor(np.array(next_states)).to(device)
                actions = torch.LongTensor(actions).unsqueeze(1).to(device)
                rewards = torch.FloatTensor(rewards).to(device)
                dones = torch.FloatTensor(dones).to(device)

                current_q_values = model(states).gather(1, actions).squeeze()
                with torch.no_grad():
                    next_q_values = target_model(next_states).max(1)[0]
                    target_q_values = rewards + (1 - dones) * gamma * next_q_values

                loss = criterion(current_q_values, target_q_values)
                optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()

        epsilon = max(epsilon * epsilon_decay, epsilon_min)

        if episode % target_update == 0:
            target_model.load_state_dict(model.state_dict())

        rewards_history.append(total_reward)
        steps_history.append(step_count)

        # Update plots
        ax1.clear()
        
        ax1.plot(rewards_history, label='Rewards', color='blue')
        ax1.set_title("Episode Rewards")
        ax1.set_xlabel("Episodes")
        ax1.set_ylabel("Total Reward")
        ax1.legend()

        plt.draw()
        plt.pause(0.01)

        if (episode + 1) % 100 == 0:
            if not episode_terminated:
                visualize_episode_steps(env, model, episode, fig, ax1)
            avg_reward = sum(rewards_history[-10:]) / 10
            print(f"Episode {episode + 1}: Average Reward: {avg_reward:.2f}, Epsilon: {epsilon:.2f}")
            if episode_terminated:
                print(f"Episode terminated early due to low reward: {total_reward}")

    plt.ioff()
    plt.show()

    return model, rewards_history

env = GridEnvironment()
trained_model = train_dqn_with_visualization(env)