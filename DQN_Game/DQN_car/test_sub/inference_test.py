import numpy as np
import random
import math
import matplotlib.pyplot as plt
import matplotlib.patches as patches  # ì¥ì• ë¬¼ ê·¸ë¦¬ê¸°ë¥¼ ìœ„í•´ ì¶”ê°€
import torch
import torch.nn as nn
import gymnasium as gym
from gymnasium import spaces


# -----------------------------
# 1. í™˜ê²½ ì •ì˜ (train.pyì™€ ë™ì¼í•˜ê²Œ ìˆ˜ì •)
# -----------------------------
class ObstacleAvoidanceEnv(gym.Env):
    """
    ì§ì‚¬ê°í˜• ê²©ì ë§µ í™˜ê²½:
     - ì—ì´ì „íŠ¸ëŠ” ë§µ ê²½ê³„ì™€ ë‚´ë¶€ì— ëœë¤í•˜ê²Œ ë°°ì¹˜ëœ ì§ì‚¬ê°í˜• ì¥ì• ë¬¼ì„ í”¼í•´ ì›€ì§ì„.
     - ìƒíƒœ: ì—ì´ì „íŠ¸ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ 5ë°©í–¥ì˜ ì¥ì• ë¬¼ ë˜ëŠ” ê²½ê³„ê¹Œì§€ ê±°ë¦¬ (ì„¼ì„œ ê°’)
     - í–‰ë™: 0 - ì „ì§„, 1 - ì¢Œì¸¡ 45ë„ íšŒì „ í›„ ì „ì§„, 2 - ìš°ì¸¡ 45ë„ íšŒì „ í›„ ì „ì§„
     - ëª©í‘œ: ì¶©ëŒ ì—†ì´ ìµœëŒ€í•œ ì˜¤ë˜ ì´ë™
    """

    metadata = {"render_modes": ["human", "rgb_array"], "render_fps": 30}

    def __init__(
        self,
        map_width=20,
        map_height=10,
        num_obstacles=10,
        min_obstacle_size=1.0,
        max_obstacle_size=5.0,
        max_steps=100,
    ):  # max_steps ì¦ê°€ ê³ ë ¤
        super(ObstacleAvoidanceEnv, self).__init__()

        # â”€â”€ ëª©í‘œ ì§€ì  ì„¤ì • â”€â”€
        self.goal_x = map_width - 0.1
        self.goal_y = map_height / 2.0
        self.goal_reward = 100.0

        self.map_width = map_width
        self.map_height = map_height
        self.num_obstacles = num_obstacles
        self.min_obstacle_size = min_obstacle_size
        self.max_obstacle_size = max_obstacle_size
        self.max_steps = max_steps

        # ì„¼ì„œ ì„¤ì •: ìƒëŒ€ê°ë„(ë„ ë‹¨ìœ„) 0, +45, -45, +90, -90
        self.sensor_angles = [0, 45, -45, 90, -90]
        self.sensor_max_range = math.sqrt(
            map_width**2 + map_height**2
        )  # ë§µ ëŒ€ê°ì„  ê¸¸ì´ë¡œ ìµœëŒ€ ë²”ìœ„ ì„¤ì •
        self.sensor_ray_step = 0.2  # ì„¼ì„œ ê°ì§€ ì •ë°€ë„

        # ì—ì´ì „íŠ¸ê°€ í•œ ë²ˆì— ì´ë™í•˜ëŠ” ê±°ë¦¬
        self.step_size = 1.0

        # í–‰ë™ ê³µê°„: 0 - ì „ì§„, 1 - ì¢Œì¸¡ 45ë„ íšŒì „, 2 - ìš°ì¸¡ 45ë„ íšŒì „
        self.action_space = spaces.Discrete(3)
        # ê´€ì¸¡ ê³µê°„: 5ê°œ ì„¼ì„œì˜ ê±°ë¦¬ê°’ (0 ~ sensor_max_range)
        self.observation_space = spaces.Box(
            low=0.0, high=self.sensor_max_range, shape=(5,), dtype=np.float32
        )

        self.obstacles = []  # ì¥ì• ë¬¼ ì •ë³´ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸ (x, y, width, height)
        self.agent_pos = np.zeros(2, dtype=np.float32)
        self.agent_heading = 0.0  # ë„ ë‹¨ìœ„
        self.num_steps = 0
        # self.done = False # Gymnasium APIì—ì„œëŠ” terminated/truncated ì‚¬ìš©
        self.terminated = False
        self.truncated = False

        # ì‹œê°í™”ë¥¼ ìœ„í•œ ë³€ìˆ˜
        self.fig = None
        self.ax = None

        # ì´ˆê¸°í™” ì‹œ reset í˜¸ì¶œ ë³´ì¥ (gym.Env ê¶Œì¥ì‚¬í•­)
        # observation, info = self.reset() # reset()ì´ observation, info íŠœí”Œì„ ë°˜í™˜í•˜ë„ë¡ ìˆ˜ì • í•„ìš”
        # ì—¬ê¸°ì„œëŠ” ì¼ë‹¨ reset ë‚´ë¶€ì—ì„œ observation ê³„ì‚°ë§Œ í•˜ë„ë¡ ìœ ì§€

    def _generate_obstacles(self):
        """ë§µ ë‚´ë¶€ì— ê²¹ì¹˜ì§€ ì•Šë„ë¡ ëœë¤ ì¥ì• ë¬¼ ìƒì„±"""
        self.obstacles = []
        attempts = 0
        max_attempts = self.num_obstacles * 20  # ì¥ì• ë¬¼ ìƒì„± ì‹œë„ íšŸìˆ˜ ì œí•œ

        while len(self.obstacles) < self.num_obstacles and attempts < max_attempts:
            attempts += 1
            # ëœë¤ í¬ê¸°
            obs_w = random.uniform(self.min_obstacle_size, self.max_obstacle_size)
            obs_h = random.uniform(self.min_obstacle_size, self.max_obstacle_size)
            # ëœë¤ ìœ„ì¹˜ (ë§µ ë‚´ë¶€ì— ì™„ì „íˆ ë“¤ì–´ì˜¤ë„ë¡)
            obs_x = random.uniform(0, self.map_width - obs_w)
            obs_y = random.uniform(0, self.map_height - obs_h)

            new_obstacle = (obs_x, obs_y, obs_w, obs_h)

            # ë‹¤ë¥¸ ì¥ì• ë¬¼ê³¼ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸ (ë‹¨ìˆœ AABB ì¶©ëŒ ê²€ì‚¬)
            collision = False
            for ox, oy, ow, oh in self.obstacles:
                if (
                    obs_x < ox + ow
                    and obs_x + obs_w > ox
                    and obs_y < oy + oh
                    and obs_y + obs_h > oy
                ):
                    collision = True
                    break

            if not collision:
                self.obstacles.append(new_obstacle)

        if len(self.obstacles) < self.num_obstacles:
            print(
                f"Warning: Could only generate {len(self.obstacles)} non-overlapping obstacles."
            )

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)  # Gymnasium API ì¤€ìˆ˜

        # ì¥ì• ë¬¼ ì¬ìƒì„±
        self._generate_obstacles()

        # ì—ì´ì „íŠ¸ë¥¼ ë¹ˆ ê³µê°„ì— ë°°ì¹˜ (ì¥ì• ë¬¼ ë° ê²½ê³„ì™€ ê²¹ì¹˜ì§€ ì•Šë„ë¡)
        valid_position = False
        while not valid_position:
            self.agent_pos = np.array(
                [
                    random.uniform(0.5, self.map_width - 0.5),  # ê²½ê³„ì—ì„œ ì•½ê°„ ì•ˆìª½
                    random.uniform(0.5, self.map_height - 0.5),
                ],
                dtype=np.float32,
            )
            if not self._is_collision(self.agent_pos):
                valid_position = True

        # ì—ì´ì „íŠ¸ì˜ ì´ˆê¸° headingì€ ë¬´ì‘ìœ„ ì„ íƒ (ë‹¨ìœ„: ë„)
        self.agent_heading = random.uniform(
            0, 360
        )  # ë” ë¶€ë“œëŸ¬ìš´ ì‹œì‘ì„ ìœ„í•´ ì—°ì†ì ì¸ ê°ë„ ì‚¬ìš© ê°€ëŠ¥

        self.num_steps = 0
        # self.done = False
        self.terminated = False
        self.truncated = False

        observation = self._get_observation()
        info = {}  # ì¶”ê°€ ì •ë³´ (í•„ìš”ì‹œ)

        # ì‹œê°í™” ì´ˆê¸°í™” (í•„ìš”ì‹œ)
        if self.fig is not None:
            plt.close(self.fig)
            self.fig = None
            self.ax = None

        return observation, info  # Gymnasium API ì¤€ìˆ˜

    def step(self, action):
        # if self.done: # ì´ì „ ë°©ì‹
        if self.terminated or self.truncated:
            # Gymnasium ìµœì‹  ë²„ì „ì—ì„œëŠ” ì—í”¼ì†Œë“œ ì¢…ë£Œ í›„ step í˜¸ì¶œ ì‹œ ê²½ê³  ë°œìƒ
            # return self._get_observation(), 0, self.done, False, {} # ì´ì „ ë°©ì‹
            # í™˜ê²½ ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì—¬ ì ì ˆí•œ ë°˜í™˜ê°’ ê²°ì • í•„ìš” (ë³´í†µ ë§ˆì§€ë§‰ ìƒíƒœ ë°˜í™˜)
            # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ë§ˆì§€ë§‰ ìƒíƒœì™€ 0 ë³´ìƒ ë°˜í™˜ ê°€ì •
            obs = self._get_observation()
            # return obs, 0.0, self.done, False, {} # truncated=False # ì´ì „ ë°©ì‹
            return obs, 0.0, self.terminated, self.truncated, {}

        # í–‰ë™ì— ë”°ë¥¸ íšŒì „ ë° ë³´ìƒ/í˜ë„í‹° ì„¤ì •
        turn_penalty = -0.1  # íšŒì „ ì‹œ í˜ë„í‹° (ì¡°ì • ê°€ëŠ¥)
        straight_bonus = 0.2  # ì§ì§„ ì‹œ ë³´ë„ˆìŠ¤ (ì¡°ì • ê°€ëŠ¥)
        action_reward = 0.0

        if action == 1:  # ì¢ŒíšŒì „
            self.agent_heading = (self.agent_heading + 45) % 360
            action_reward = turn_penalty
        elif action == 2:  # ìš°íšŒì „
            self.agent_heading = (self.agent_heading - 45) % 360
            action_reward = turn_penalty
        else:  # ì§ì§„ (action == 0)
            action_reward = straight_bonus

        # ì—ì´ì „íŠ¸ ì´ë™ (step_size ë§Œí¼)
        rad = math.radians(self.agent_heading)
        delta = np.array([math.cos(rad), math.sin(rad)]) * self.step_size
        new_pos = self.agent_pos + delta

        # ì´ë™ í›„ ì¶©ëŒ ì²´í¬
        collision = self._is_collision(new_pos)
        # truncated = False # ì‹œê°„ ì´ˆê³¼ ì™¸ì˜ ì´ìœ ë¡œ ì¢…ë£Œë˜ì§€ ì•ŠìŒ # step ì‹œì‘ ì‹œ ì´ˆê¸°í™”
        # ëª©í‘œ ë„ë‹¬ ì²´í¬: ì¶©ëŒ ì—†ì´ x â‰¥ goal_x ì¼ ë•Œ
        if not collision and new_pos[0] >= self.goal_x:
            self.agent_pos = new_pos
            observation = self._get_observation()
            return observation, self.goal_reward, True, False, {}

        if collision:
            reward = -100.0  # ì¶©ëŒ ì‹œ í° í˜ë„í‹°
            # self.done = True
            self.terminated = True
        else:
            self.agent_pos = new_pos
            # ìƒì¡´ ë³´ìƒ + í–‰ë™ ë³´ìƒ (ì§ì§„ ë³´ë„ˆìŠ¤ ë˜ëŠ” íšŒì „ í˜ë„í‹°)
            reward = 1.0 + action_reward

        self.num_steps += 1
        if self.num_steps >= self.max_steps:
            # self.done = True
            self.truncated = True  # ì‹œê°„ ì´ˆê³¼ë¡œ ì¸í•œ ì¢…ë£Œ í‘œì‹œ
            if (
                not collision
            ):  # ì‹œê°„ ì´ˆê³¼ ì‹œ ì¶©ëŒì´ ì•„ë‹ˆì—ˆë‹¤ë©´, ë§ˆì§€ë§‰ ìŠ¤í… ë³´ìƒì—ì„œ í˜ë„í‹°/ë³´ë„ˆìŠ¤ ì œì™¸ ê°€ëŠ¥ (ì„ íƒ ì‚¬í•­)
                reward = 1.0  # ì˜ˆ: ì‹œê°„ ì´ˆê³¼ ìì²´ëŠ” í˜ë„í‹°ê°€ ì•„ë‹˜

        observation = self._get_observation()
        info = {}

        # return observation, reward, self.done, truncated, info # Gymnasium API ì¤€ìˆ˜ (obs, rew, terminated, truncated, info)
        return observation, reward, self.terminated, self.truncated, info

    def _is_collision(self, pos):
        # ë§µ ê²½ê³„ ì²´í¬
        if not (0 <= pos[0] < self.map_width and 0 <= pos[1] < self.map_height):
            return True

        # ê° ì¥ì• ë¬¼ê³¼ ì¶©ëŒ ì²´í¬ (AABB ì¶©ëŒ ê²€ì‚¬)
        for obs_x, obs_y, obs_w, obs_h in self.obstacles:
            if obs_x <= pos[0] < obs_x + obs_w and obs_y <= pos[1] < obs_y + obs_h:
                return True
        return False

    def _get_observation(self):
        # ì—ì´ì „íŠ¸ ìœ„ì¹˜ ê¸°ì¤€, ê° ì„¼ì„œ ë°©í–¥ìœ¼ë¡œ ê²½ê³„ ë˜ëŠ” ì¥ì• ë¬¼ê¹Œì§€ì˜ ê±°ë¦¬ ê³„ì‚°
        sensor_readings = []
        for angle_offset in self.sensor_angles:
            sensor_angle_deg = (self.agent_heading + angle_offset) % 360
            sensor_angle_rad = math.radians(sensor_angle_deg)
            direction = np.array(
                [math.cos(sensor_angle_rad), math.sin(sensor_angle_rad)]
            )

            distance = 0.0
            hit = False
            # ì„¼ì„œ ìµœëŒ€ ë²”ìœ„ê¹Œì§€ ì¡°ê¸ˆì”© ì „ì§„í•˜ë©° ì¶©ëŒ ê²€ì‚¬
            while distance < self.sensor_max_range:
                test_pos = self.agent_pos + direction * distance
                if self._is_collision(test_pos):
                    hit = True
                    break
                distance += self.sensor_ray_step  # ì •ë°€ë„ì— ë”°ë¼ ì¡°ì ˆ

            # ì‹¤ì œ ì¶©ëŒ ê±°ë¦¬ ë˜ëŠ” ìµœëŒ€ ë²”ìœ„ ì¤‘ ì‘ì€ ê°’ ì‚¬ìš©
            sensor_readings.append(min(distance, self.sensor_max_range))

        return np.array(sensor_readings, dtype=np.float32)

    def render(self, mode="human"):
        # ì‹œê°í™” ì„¤ì • (ìµœì´ˆ í˜¸ì¶œ ì‹œ)
        if self.fig is None or self.ax is None:
            self.fig, self.ax = plt.subplots(figsize=(8, 8))  # í¬ê¸° ì¡°ì ˆ ê°€ëŠ¥
            if mode == "human":
                plt.ion()  # ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ í™œì„±í™” (human ëª¨ë“œì—ì„œë§Œ)

        self.ax.clear()  # ì´ì „ í”„ë ˆì„ ì§€ìš°ê¸°

        # ë§µ ê²½ê³„ ì„¤ì • ë° ë°°ê²½
        self.ax.set_xlim(0, self.map_width)
        self.ax.set_ylim(0, self.map_height)
        self.ax.set_aspect("equal", adjustable="box")
        self.ax.set_facecolor("lightgray")  # ë°°ê²½ìƒ‰

        # ì¥ì• ë¬¼ ê·¸ë¦¬ê¸°
        for x, y, w, h in self.obstacles:
            rect = patches.Rectangle(
                (x, y), w, h, linewidth=1, edgecolor="black", facecolor="dimgray"
            )
            self.ax.add_patch(rect)

        # ì—ì´ì „íŠ¸ ê·¸ë¦¬ê¸° (íŒŒë€ìƒ‰ ì›)
        self.ax.plot(
            self.agent_pos[0], self.agent_pos[1], "bo", markersize=8, label="Agent"
        )

        # â”€â”€ ëª©í‘œ ì§€ì  â˜… í‘œì‹œ â”€â”€
        self.ax.plot(
            self.goal_x,
            self.goal_y,
            marker="*",
            markersize=20,
            color="gold",
            label="Goal",
        )

        # # ì„¼ì„œ ê´‘ì„  ê·¸ë¦¬ê¸° (ë¹¨ê°„ ì ì„ )
        # obs_for_render = self._get_observation()  # í˜„ì¬ ì„¼ì„œ ê°’ ê°€ì ¸ì˜¤ê¸°
        # for i, angle_offset in enumerate(self.sensor_angles):
        #     sensor_angle_deg = (self.agent_heading + angle_offset) % 360
        #     sensor_angle_rad = math.radians(sensor_angle_deg)
        #     distance = obs_for_render[i]  # ê³„ì‚°ëœ ê±°ë¦¬ ì‚¬ìš©
        #     end_point = (
        #         self.agent_pos
        #         + np.array([math.cos(sensor_angle_rad), math.sin(sensor_angle_rad)])
        #         * distance
        #     )
        #     self.ax.plot(
        #         [self.agent_pos[0], end_point[0]],
        #         [self.agent_pos[1], end_point[1]],
        #         "r--",
        #         linewidth=1,
        #         label="Sensor Ray" if i == 0 else "",
        #     )  # ë²”ë¡€ ì¤‘ë³µ ë°©ì§€

        # ì œëª© ë° ë ˆì´ì•„ì›ƒ
        self.ax.set_title(f"Obstacle Avoidance | Step: {self.num_steps}")
        # self.ax.legend() # ë²”ë¡€ê°€ ë„ˆë¬´ ë§ì•„ì§ˆ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬
        plt.tight_layout()

        if mode == "human":
            plt.pause(0.01)  # ì ì‹œ ë©ˆì¶°ì„œ ë³´ì—¬ì¤Œ
            self.fig.canvas.flush_events()  # ì´ë²¤íŠ¸ ì²˜ë¦¬ ë³´ì¥
            self.fig.canvas.draw()  # ìº”ë²„ìŠ¤ ì—…ë°ì´íŠ¸

        elif mode == "rgb_array":
            self.fig.canvas.draw()
            renderer = self.fig.canvas.get_renderer()
            img = np.frombuffer(renderer.tostring_rgb(), dtype=np.uint8)
            img = img.reshape(self.fig.canvas.get_width_height()[::-1] + (3,))
            # plt.close(self.fig) # rgb_array ëª¨ë“œì—ì„œëŠ” ê³„ì† ì‚¬ìš©í•´ì•¼ í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë‹«ì§€ ì•ŠìŒ (í•„ìš”ì‹œ ê´€ë¦¬)
            return img

    def close(self):
        # ì‹œê°í™” ì°½ ë‹«ê¸°
        if self.fig is not None:
            plt.ioff()  # ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ ë¹„í™œì„±í™”
            plt.close(self.fig)
            self.fig = None
            self.ax = None


# -----------------------------
# 2. DQN ëª¨ë¸ ë° ì—ì´ì „íŠ¸ ì •ì˜ (train.pyì™€ ë™ì¼í•œ êµ¬ì¡° ì‚¬ìš©)
# -----------------------------
class DQN(nn.Module):
    def __init__(self, input_dim=5, output_dim=3):
        super(DQN, self).__init__()
        # train.pyì˜ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ì™€ ë™ì¼í•˜ê²Œ ë§ì¶°ì•¼ í•¨
        self.net = nn.Sequential(
            nn.Linear(input_dim, 128),  # train.pyì™€ ë™ì¼í•˜ê²Œ 128
            nn.ReLU(),
            nn.Linear(128, 128),  # train.pyì™€ ë™ì¼í•˜ê²Œ 128
            nn.ReLU(),
            nn.Linear(128, 64),  # train.pyì™€ ë™ì¼í•˜ê²Œ 64
            nn.ReLU(),
            nn.Linear(64, output_dim),
        )

    def forward(self, x):
        return self.net(x)


class DQNAgent:
    def __init__(self, state_dim=5, action_dim=3):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        # DQN í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ policy_net ì´ˆê¸°í™”
        self.policy_net = DQN(input_dim=state_dim, output_dim=action_dim).to(
            self.device
        )
        # ì¶”ë¡  ì‹œì—ëŠ” eval ëª¨ë“œë¡œ ì„¤ì •
        self.policy_net.eval()

    def select_action(self, state):
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        with torch.no_grad():
            q_values = self.policy_net(state_tensor)
            # ê°€ì¥ ë†’ì€ Q ê°’ì„ ê°€ì§„ í–‰ë™ ì„ íƒ
            return q_values.max(1)[1].item()


# -----------------------------
# 3. ì• ë‹ˆë©”ì´ì…˜ ë°©ì‹ ì‹œë®¬ë ˆì´ì…˜ (simulate_live) - ìˆ˜ì •ë¨
# -----------------------------
def simulate_live(agent, max_steps=100):
    # train.pyì™€ ë™ì¼í•œ í™˜ê²½ íŒŒë¼ë¯¸í„° ì‚¬ìš©
    env = ObstacleAvoidanceEnv(
        map_width=20, map_height=10, num_obstacles=15, max_steps=max_steps
    )
    state, _ = env.reset()  # infoëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
    total_reward = 0
    terminated = False
    truncated = False

    # plt.ion() ë“±ì€ env.render() ë‚´ë¶€ì—ì„œ ì²˜ë¦¬ë¨

    while not (terminated or truncated):
        action = agent.select_action(state)
        # Gymnasium APIì— ë§ê²Œ ë°˜í™˜ê°’ ë°›ê¸°
        next_state, reward, terminated, truncated, _ = env.step(
            action
        )  # infoëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
        total_reward += reward

        if reward == env.goal_reward:
            print("ğŸ Goal reached! Final reward:", total_reward)
            break

        # í™˜ê²½ì˜ render ë©”ì„œë“œ ì‚¬ìš©
        env.render(mode="human")

        state = next_state
        # done í”Œë˜ê·¸ ëŒ€ì‹  terminatedì™€ truncated ì‚¬ìš©
        if terminated or truncated:
            break

    # plt.ioff() ë“±ì€ env.close() ë‚´ë¶€ì—ì„œ ì²˜ë¦¬ë¨
    env.close()  # ì‹œë®¬ë ˆì´ì…˜ ì¢…ë£Œ í›„ í™˜ê²½ ìì› í•´ì œ
    print(f"ì‹œë®¬ë ˆì´ì…˜ ì¢…ë£Œ - ì´ ë³´ìƒ: {total_reward}")


# -----------------------------
# 4. ë©”ì¸ ì‹¤í–‰ë¶€ (ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰)
# -----------------------------
if __name__ == "__main__":
    # DQNAgent ì´ˆê¸°í™” (state_dim, action_dim í™•ì¸)
    agent = DQNAgent(state_dim=5, action_dim=3)
    # train.pyì—ì„œ ì €ì¥í•œ ê°€ì¤‘ì¹˜ íŒŒì¼ëª… ì‚¬ìš©
    weights_path = "dqn_obstacle_avoidance_weights_test.pth"
    try:
        # map_locationì„ ì‚¬ìš©í•˜ì—¬ CPU/GPU ê°„ í˜¸í™˜ì„± í™•ë³´
        agent.policy_net.load_state_dict(
            torch.load(weights_path, map_location=agent.device)
        )
        print(f"ê°€ì¤‘ì¹˜ íŒŒì¼ '{weights_path}'ì„(ë¥¼) ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí•˜ì˜€ìŠµë‹ˆë‹¤.")
    except FileNotFoundError:
        print(f"ì˜¤ë¥˜: ê°€ì¤‘ì¹˜ íŒŒì¼ '{weights_path}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        exit()
    except Exception as e:
        print(f"ê°€ì¤‘ì¹˜ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        exit()

    # ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰ (max_stepsëŠ” í™˜ê²½ ì„¤ì •ê³¼ ë§ì¶¤)
    simulate_live(agent, max_steps=100)
